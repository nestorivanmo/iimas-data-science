{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tarea 6 - Extracción de Características\n",
    "\n",
    "- Martínez Ostoa Néstor\n",
    "- Minería de Textos\n",
    "- IIMAS, UNAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparación de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5572, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>FreeMsg Hey there darling it's been 3 week's n...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Even my brother is not like to speak with me. ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>As per your request 'Melle Melle (Oru Minnamin...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>WINNER!! As a valued network customer you have...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Had your mobile 11 months or more? U R entitle...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  target\n",
       "0  Go until jurong point, crazy.. Available only ...       0\n",
       "1                      Ok lar... Joking wif u oni...       0\n",
       "2  Free entry in 2 a wkly comp to win FA Cup fina...       1\n",
       "3  U dun say so early hor... U c already then say...       0\n",
       "4  Nah I don't think he goes to usf, he lives aro...       0\n",
       "5  FreeMsg Hey there darling it's been 3 week's n...       1\n",
       "6  Even my brother is not like to speak with me. ...       0\n",
       "7  As per your request 'Melle Melle (Oru Minnamin...       0\n",
       "8  WINNER!! As a valued network customer you have...       1\n",
       "9  Had your mobile 11 months or more? U R entitle...       1"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "spam_data = pd.read_csv('spam.csv')\n",
    "\n",
    "print(spam_data.shape)\n",
    "\n",
    "spam_data['target'] = np.where(spam_data['target']=='spam',1,0)\n",
    "spam_data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## División de muestras en entrenamiento (train) y prueba (test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4179\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(spam_data['text'], \n",
    "                                                    spam_data['target'], \n",
    "                                                    random_state=0)\n",
    "print(len(X_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pregunta 1\n",
    "\n",
    "Ajustar los datos de entrenamiento `X_train` utilizando un `count_vectorizer` con parámetros predeterminados.\n",
    "\n",
    "¿Cuál es el token más largo en el vocabulario?\n",
    "\n",
    "*Esta función debería devolver una cadena.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'com1win150ppmx3age16subscription'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def respuesta_uno (X_train):\n",
    "    vectorizer = CountVectorizer(lowercase=True)\n",
    "    vectorizer.fit_transform(X_train)\n",
    "    \n",
    "    largest_token = \"\"\n",
    "    for token in vectorizer.get_feature_names_out():\n",
    "        if len(token) > len(largest_token):\n",
    "            largest_token = token\n",
    "            \n",
    "    return largest_token\n",
    "\n",
    "respuesta_uno(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88039.SkilGme.TsCs087147403231Winawk!Age16+å£1.50perWKsub\n"
     ]
    }
   ],
   "source": [
    "lt = \"\"\n",
    "for i in range(X_train.shape[0]):\n",
    "    text = X_train.iloc[i]\n",
    "    text = text.split()\n",
    "    for w in text:\n",
    "        if len(w) > len(lt):\n",
    "            lt = w\n",
    "            \n",
    "print(lt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pregunta 2\n",
    "\n",
    "¿Cuál es el número promedio de caracteres por documento para los documentos no spam y spam?\n",
    "\n",
    "*Esta función debe devolver una tupla (promedio de # caracteres no es spam, promedio # caracteres spam).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(138.8661, 71.0236)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def avg_token(text):\n",
    "    lengths = []\n",
    "    for word in text:\n",
    "        lengths.append(len(word))\n",
    "\n",
    "    lengths = np.array(lengths)\n",
    "    return np.round(np.sum(lengths) / lengths.shape[0], 4)\n",
    "\n",
    "def respuesta_dos(spam_data):\n",
    "    X_spam = spam_data[spam_data['target'] == 1]\n",
    "    X_ham = spam_data[spam_data['target'] == 0]\n",
    "    \n",
    "    assert X_spam['target'].unique()[0] == 1\n",
    "    assert X_ham['target'].unique()[0] == 0\n",
    "    \n",
    "    return (avg_token(X_spam['text']), avg_token(X_ham['text']))\n",
    "\n",
    "respuesta_dos(spam_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pregunta 3\n",
    "\n",
    "¿Cuál es el número promedio de dígitos por documento para los documentos no spam y spam?\n",
    "\n",
    "*Esta función debe devolver una tupla (promedio de # dígitos no es spam, promedio # dígitos spam).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15.759, 0.2993)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def avg_digit(text):\n",
    "    num_docs = text.shape[0]\n",
    "    digits_per_doc = []\n",
    "    \n",
    "    for word in text:\n",
    "        digits = 0\n",
    "        for c in word:\n",
    "            if c.isdigit():\n",
    "                digits += 1\n",
    "        digits_per_doc.append(digits)\n",
    "    \n",
    "    return np.round(np.sum(np.array(digits_per_doc)) / num_docs, 4)\n",
    "\n",
    "\n",
    "def respuesta_tres(spam_data):\n",
    "    X_spam = spam_data[spam_data['target'] == 1]\n",
    "    X_ham = spam_data[spam_data['target'] == 0]\n",
    "    \n",
    "    assert X_spam['target'].unique()[0] == 1\n",
    "    assert X_ham['target'].unique()[0] == 0\n",
    "    \n",
    "    return (avg_digit(X_spam['text']), avg_digit(X_ham['text']))\n",
    "    \n",
    "respuesta_tres(spam_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pregunta 4\n",
    "\n",
    "¿Cuál es el número promedio de caracteres que no son palabras (cualquier cosa que no sea una letra, un dígito o un guión bajo) por documento para los documentos que no son spam y spam?\n",
    "\n",
    "*Sugerencia: utilice las clases de caracteres `\\ w` y` \\ W`*\n",
    "\n",
    "*Esta función debe devolver una tupla (promedio de # caracteres que no son palabras, no spam, promedio de # caracteres que no son palabras, spam).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17.29181347150259, 29.041499330655956)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re \n",
    "\n",
    "def pregunta_cuatro(spam_data):\n",
    "    temp = spam_data.copy()\n",
    "    temp['non_words'] = temp['text'].apply(lambda x: len(re.findall(r'\\W', x)))\n",
    "    \n",
    "    return (\n",
    "        temp[temp.target == 0]['non_words'].mean(), \n",
    "        temp[temp.target == 1]['non_words'].mean()\n",
    "    )\n",
    "\n",
    "pregunta_cuatro(spam_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pregunta 5\n",
    "\n",
    "¿Cuál es el tamaño del vocabulario en `X_train` y `X_test`, primero utilizando la función `fit_transform` en ambos (train y test), luego utilizando `fit_transform` sobre el train y solo `transform` en el test\n",
    "\n",
    "\n",
    "\n",
    "*Esta función debe devolver dos tuplas una con `fit_transform` y la otra con `transform` (vocabulario en `X_train`, vocabulario en `X_test`), (vocabulario en `X_train`, vocabulario en `X_test`).*.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7354, 4170, 7354, 7354)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def respuesta_cinco(X_train, X_test):\n",
    "    cv1 = CountVectorizer()\n",
    "    X1 = cv1.fit_transform(X_train)\n",
    "    cv2 = CountVectorizer()\n",
    "    X2 = cv2.fit_transform(X_test)\n",
    "    \n",
    "    cv3 = CountVectorizer()\n",
    "    X3 = cv3.fit_transform(X_train)\n",
    "    X4 = cv3.transform(X_test)\n",
    "    \n",
    "    return (X1.shape[1], X2.shape[1], X3.shape[1], X4.shape[1])\n",
    "    \n",
    "respuesta_cinco(X_train, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pregunta 6\n",
    "\n",
    "¿Cuales son las 10 palabras mas frecuentes (sin tener en cuenta *Stopwords*) en los documentos que no son spam y spam?\n",
    "\n",
    "\n",
    "*Esta función debe devolver una tupla (palabras mas frecuentes, no spam, palabras mas frecuentes, spam).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([\"i'm\",\n",
       "  'get',\n",
       "  '&lt;#&gt;',\n",
       "  'got',\n",
       "  'like',\n",
       "  'call',\n",
       "  'come',\n",
       "  'know',\n",
       "  'you',\n",
       "  'good'],\n",
       " ['call',\n",
       "  'free',\n",
       "  'txt',\n",
       "  'text',\n",
       "  'mobile',\n",
       "  'claim',\n",
       "  'reply',\n",
       "  'stop',\n",
       "  'you',\n",
       "  'get'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def most_common(spam_data, target, k):\n",
    "    d = '\\n'.join(spam_data[spam_data['target'] == target]['text'].values).split()\n",
    "    c = Counter(d)\n",
    "    return [t[0] for t in c.most_common(k)]\n",
    "\n",
    "def pregunta_seis(spam_data, stop_words, k=10):\n",
    "    spam_data = spam_data.copy()\n",
    "    spam_data['text'] = spam_data['text'].apply(lambda x : ' '.join(w.lower() for w in x.split() if w not in stop_words and len(w) > 2))\n",
    "    \n",
    "    return (most_common(spam_data, 0, k), most_common(spam_data, 1, k))\n",
    "\n",
    "pregunta_seis(spam_data, stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pregunta 7\n",
    "\n",
    "¿Cuales son las 10 palabras mas frecuentes (solo teniendo en cuenta *Stopwords*) en los documentos que no son spam y spam?\n",
    "\n",
    "\n",
    "*Esta función debe devolver una tupla (palabras mas frecuentes, no spam, palabras mas frecuentes, spam).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['to', 'you', 'the', 'a', 'and', 'i', 'in', 'is', 'my', 'me'],\n",
       " ['to', 'a', 'your', 'or', 'the', 'for', 'you', 'is', 'on', 'have'])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def pregunta_siete(spam_data, stop_words, k=10):\n",
    "    spam_data = spam_data.copy()\n",
    "    spam_data['text'] = spam_data['text'].apply(\n",
    "        lambda x : ' '.join(\n",
    "            w for w in x.split() if w in stop_words\n",
    "        )\n",
    "    )\n",
    "    return (most_common(spam_data, 0, k), most_common(spam_data, 1, k))\n",
    "\n",
    "pregunta_siete(spam_data, stop_words)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
