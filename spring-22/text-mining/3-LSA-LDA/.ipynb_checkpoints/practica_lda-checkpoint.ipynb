{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trabajando con Gensim - LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparación de documentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1 = \"Sugar is bad to consume. My sister likes to have sugar, but not my father.\"\n",
    "doc2 = \"My father spends a lot of time driving my sister around to dance practice.\"\n",
    "doc3 = \"Doctors suggest that driving may cause increased stress and blood pressure.\"\n",
    "doc4 = \"Sometimes I feel pressure to perform well at school, but my father never seems to drive my sister to do better.\"\n",
    "doc5 = \"Health experts say that Sugar is not good for your lifestyle.\"\n",
    "# compilar documentos\n",
    "doc_complete = [doc1, doc2, doc3, doc4, doc5]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-procesamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[u'sugar', u'bad', u'consume', u'sister', u'like', u'sugar', u'father'],\n",
       " ['father',\n",
       "  'spends',\n",
       "  'lot',\n",
       "  'time',\n",
       "  'driving',\n",
       "  'sister',\n",
       "  'around',\n",
       "  'dance',\n",
       "  'practice'],\n",
       " [u'doctor',\n",
       "  u'suggest',\n",
       "  u'driving',\n",
       "  u'may',\n",
       "  u'cause',\n",
       "  u'increased',\n",
       "  u'stress',\n",
       "  u'blood',\n",
       "  u'pressure'],\n",
       " ['sometimes',\n",
       "  'feel',\n",
       "  'pressure',\n",
       "  'perform',\n",
       "  'well',\n",
       "  'school',\n",
       "  'father',\n",
       "  'never',\n",
       "  'seems',\n",
       "  'drive',\n",
       "  'sister',\n",
       "  'better'],\n",
       " [u'health', u'expert', u'say', u'sugar', u'good', u'lifestyle']]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords \n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import string\n",
    "stop = set(stopwords.words('english'))\n",
    "exclude = set(string.punctuation) \n",
    "lemma = WordNetLemmatizer()\n",
    "def clean(doc):\n",
    "    stop_free = \" \".join([i for i in doc.lower().split() if i not in stop])\n",
    "    punc_free = ''.join(ch for ch in stop_free if ch not in exclude)\n",
    "    normalized = \" \".join(lemma.lemmatize(word) for word in punc_free.split())\n",
    "    return normalized\n",
    "\n",
    "doc_clean = [clean(doc).split() for doc in doc_complete]  \n",
    "doc_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matriz término-documento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(35 unique tokens: [u'expert', u'consume', u'dance', u'seems', u'say']...)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 2)],\n",
       " [(2, 1), (4, 1), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1), (11, 1), (12, 1)],\n",
       " [(8, 1),\n",
       "  (13, 1),\n",
       "  (14, 1),\n",
       "  (15, 1),\n",
       "  (16, 1),\n",
       "  (17, 1),\n",
       "  (18, 1),\n",
       "  (19, 1),\n",
       "  (20, 1)],\n",
       " [(2, 1),\n",
       "  (4, 1),\n",
       "  (18, 1),\n",
       "  (21, 1),\n",
       "  (22, 1),\n",
       "  (23, 1),\n",
       "  (24, 1),\n",
       "  (25, 1),\n",
       "  (26, 1),\n",
       "  (27, 1),\n",
       "  (28, 1),\n",
       "  (29, 1)],\n",
       " [(5, 1), (30, 1), (31, 1), (32, 1), (33, 1), (34, 1)]]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim import corpora\n",
    "\n",
    "# Creación del diccionario de términos de nuestro courpus, donde se asigna un índice a cada término único.\n",
    "dictionary = corpora.Dictionary(doc_clean)\n",
    "\n",
    "# Convertir la lista de documentos (corpus) en la Matriz de Términos del Documento utilizando el diccionario.\n",
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]\n",
    "print (dictionary)\n",
    "doc_term_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejecutar modelo LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creación del objeto para el modelo LDA usando la librería gensim\n",
    "Lda = gensim.models.ldamodel.LdaModel\n",
    "\n",
    "# Ejecución y entrenamiento del modelo LDA en la matriz de términos del documento.\n",
    "ldamodel = Lda(doc_term_matrix, num_topics=3, id2word = dictionary, passes=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, u'0.029*\"sister\" + 0.029*\"father\" + 0.029*\"driving\"'), (1, u'0.099*\"sugar\" + 0.040*\"suggest\" + 0.040*\"blood\"'), (2, u'0.072*\"father\" + 0.072*\"sister\" + 0.041*\"pressure\"')]\n"
     ]
    }
   ],
   "source": [
    "print(ldamodel.print_topics(num_topics=3, num_words=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Caso de estudio de corpus de noticias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leer el corpus y extraer características"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 901)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import gensim\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Cargar lista de documentos\n",
    "with open('newsgroups', 'rb') as f:\n",
    "    newsgroup_data = pickle.load(f)\n",
    "\n",
    "# Usar CountVectorizer para encontrar tokens de tres letras,eliminar palabras cerradas,\n",
    "# eliminar tokens que aparecen en al menos 20 documentos,\n",
    "# eliminar tokens que no aparecen en más del 20% de los documentos\n",
    "vect = CountVectorizer(min_df=20, max_df=0.2, stop_words='english', \n",
    "                       token_pattern='(?u)\\\\b\\\\w\\\\w\\\\w+\\\\b')\n",
    "# Ajustamos y transformamos\n",
    "X = vect.fit_transform(newsgroup_data)\n",
    "\n",
    "# Convertir la matriz dispersa al corpus gensim.\n",
    "corpus = gensim.matutils.Sparse2Corpus(X, documents_columns=False)\n",
    "\n",
    "# Asignación de ID de palabra a palabras (para usar en el parámetro id2word de LdaModel)\n",
    "id_map = dict((v, k) for k, v in vect.vocabulary_.items())\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usamos el constructor gensim.models.ldamodel.LdaModel para estimar\n",
    "# los parámetros del modelo LDA en el corpus, y guardar en la variable `ldamodel`\n",
    "\n",
    "ldamodel = gensim.models.ldamodel.LdaModel (corpus, num_topics=10, id2word=id_map, passes=25, random_state=34)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.056*\"edu\" + 0.043*\"com\" + 0.033*\"thanks\" + 0.022*\"mail\" + 0.021*\"know\" + 0.020*\"does\" + 0.014*\"info\" + 0.012*\"monitor\" + 0.010*\"looking\" + 0.010*\"don\"'),\n",
       " (1,\n",
       "  '0.024*\"ground\" + 0.018*\"current\" + 0.018*\"just\" + 0.013*\"want\" + 0.013*\"use\" + 0.011*\"using\" + 0.011*\"used\" + 0.010*\"power\" + 0.010*\"speed\" + 0.010*\"output\"'),\n",
       " (2,\n",
       "  '0.061*\"drive\" + 0.042*\"disk\" + 0.033*\"scsi\" + 0.030*\"drives\" + 0.028*\"hard\" + 0.028*\"controller\" + 0.027*\"card\" + 0.020*\"rom\" + 0.018*\"floppy\" + 0.017*\"bus\"'),\n",
       " (3,\n",
       "  '0.023*\"time\" + 0.015*\"atheism\" + 0.014*\"list\" + 0.013*\"left\" + 0.012*\"alt\" + 0.012*\"faq\" + 0.012*\"probably\" + 0.011*\"know\" + 0.011*\"send\" + 0.010*\"months\"'),\n",
       " (4,\n",
       "  '0.025*\"car\" + 0.016*\"just\" + 0.014*\"don\" + 0.014*\"bike\" + 0.012*\"good\" + 0.011*\"new\" + 0.011*\"think\" + 0.010*\"year\" + 0.010*\"cars\" + 0.010*\"time\"'),\n",
       " (5,\n",
       "  '0.030*\"game\" + 0.027*\"team\" + 0.023*\"year\" + 0.017*\"games\" + 0.016*\"play\" + 0.012*\"season\" + 0.012*\"players\" + 0.012*\"win\" + 0.011*\"hockey\" + 0.011*\"good\"'),\n",
       " (6,\n",
       "  '0.017*\"information\" + 0.014*\"help\" + 0.014*\"medical\" + 0.012*\"new\" + 0.012*\"use\" + 0.012*\"000\" + 0.012*\"research\" + 0.011*\"university\" + 0.010*\"number\" + 0.010*\"program\"'),\n",
       " (7,\n",
       "  '0.022*\"don\" + 0.021*\"people\" + 0.018*\"think\" + 0.017*\"just\" + 0.012*\"say\" + 0.011*\"know\" + 0.011*\"does\" + 0.011*\"good\" + 0.010*\"god\" + 0.009*\"way\"'),\n",
       " (8,\n",
       "  '0.034*\"use\" + 0.023*\"apple\" + 0.020*\"power\" + 0.016*\"time\" + 0.015*\"data\" + 0.015*\"software\" + 0.012*\"pin\" + 0.012*\"memory\" + 0.012*\"simms\" + 0.012*\"port\"'),\n",
       " (9,\n",
       "  '0.068*\"space\" + 0.036*\"nasa\" + 0.021*\"science\" + 0.020*\"edu\" + 0.019*\"data\" + 0.017*\"shuttle\" + 0.015*\"launch\" + 0.015*\"available\" + 0.014*\"center\" + 0.014*\"sci\"')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retorna una lista de los 10 topicos y las 10 palabras más significativas en cada topico.\n",
    "def lda_topics():\n",
    "    topics=ldamodel.print_topics(num_topics=10, num_words=10)\n",
    "    return topics\n",
    "lda_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribución de tópicos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0.02000183),\n",
       " (1, 0.020002047),\n",
       " (2, 0.020000001),\n",
       " (3, 0.49662152),\n",
       " (4, 0.020002762),\n",
       " (5, 0.020002853),\n",
       " (6, 0.020001693),\n",
       " (7, 0.020001365),\n",
       " (8, 0.020001847),\n",
       " (9, 0.34336406)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def topic_distribution():\n",
    "    # Transformar documento a vector\n",
    "    Xnew = vect.transform(new_doc)\n",
    "\n",
    "    # Convertir matriz dispersa a corpus.\n",
    "    newcorpus = gensim.matutils.Sparse2Corpus(Xnew, documents_columns=False)\n",
    "    \n",
    "    # distribucion de topicos\n",
    "    topic_dis = list(ldamodel[newcorpus])[0] \n",
    "    \n",
    "    return topic_dis\n",
    "\n",
    "new_doc = [\"\\n\\nIt's my understanding that the freezing will start to occur because \\\n",
    "of the\\ngrowing distance of Pluto and Charon from the Sun, due to it's\\nelliptical orbit. \\\n",
    "It is not due to shadowing effects. \\n\\n\\nPluto can shadow Charon, and vice-versa.\\n\\nGeorge \\\n",
    "Krumins\\n-- \"]\n",
    "\n",
    "topic_distribution()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
