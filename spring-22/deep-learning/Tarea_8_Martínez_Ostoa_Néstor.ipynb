{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<center><h1>Tarea 8: Encoder-Decoder</h1></center>\n<center><h4>Martínez Ostoa Néstor Iván</h4></center>\n<center><h4>Aprendizaje Profundo</h4></center>\n<center><h4><i>LCD - IIMAS - UNAM</i></h4></center>\n<center><h4>28 de marzo del 2022</h4></center>\n\n---\n\nTraducción automática con una red Encoder-Decoder","metadata":{"id":"ZgwZXA-ujlLa"}},{"cell_type":"markdown","source":"## Bibilotecas a usar","metadata":{"id":"6J1boXxOoVr8"}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch import optim\nimport torch.nn.functional as F\nimport random\nimport spacy\nimport pandas as pd\n\nfrom torchtext.legacy.datasets import Multi30k\nfrom torchtext.legacy.data import Field, BucketIterator, TabularDataset\nfrom torchtext.data.metrics import bleu_score\n\nfrom sklearn.model_selection import train_test_split","metadata":{"id":"iyISNfLYjXTe","execution":{"iopub.status.busy":"2022-03-28T18:52:49.316564Z","iopub.execute_input":"2022-03-28T18:52:49.317115Z","iopub.status.idle":"2022-03-28T18:52:49.322377Z","shell.execute_reply.started":"2022-03-28T18:52:49.317073Z","shell.execute_reply":"2022-03-28T18:52:49.321682Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"## Conjuntos de datos\n\nRepositorio con los datos utilizados en este notebook: \n- [Train Data](https://github.com/nestorivanmo/datasets/blob/master/tab-delimited-bilingual-sentence-pairs/train.csv)\n- [Validation Data](https://github.com/nestorivanmo/datasets/blob/master/tab-delimited-bilingual-sentence-pairs/validation.csv)\n- [Test Data](https://github.com/nestorivanmo/datasets/blob/master/tab-delimited-bilingual-sentence-pairs/test.csv)","metadata":{"id":"Dq7AtNcwpUAE"}},{"cell_type":"code","source":"!python -m spacy download en --quiet\n!python -m spacy download es --quiet","metadata":{"execution":{"iopub.status.busy":"2022-03-28T18:52:49.333579Z","iopub.execute_input":"2022-03-28T18:52:49.334085Z","iopub.status.idle":"2022-03-28T18:53:19.159703Z","shell.execute_reply.started":"2022-03-28T18:52:49.334054Z","shell.execute_reply":"2022-03-28T18:53:19.158822Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"spacy_english = spacy.load(\"en_core_web_sm\")\nspacy_spanish = spacy.load(\"es_core_news_sm\")","metadata":{"id":"XmxP5twbperk","execution":{"iopub.status.busy":"2022-03-28T18:53:19.161787Z","iopub.execute_input":"2022-03-28T18:53:19.162039Z","iopub.status.idle":"2022-03-28T18:53:20.212142Z","shell.execute_reply.started":"2022-03-28T18:53:19.162011Z","shell.execute_reply":"2022-03-28T18:53:20.211387Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"def tokenize_spanish(text):\n  return [token.text for token in spacy_spanish.tokenizer(text)]\n\ndef tokenize_english(text):\n  return [token.text for token in spacy_english.tokenizer(text)]\n\nspanish = Field(\n    tokenize=tokenize_spanish,\n    lower=True,\n    init_token=\"<sos>\",\n    eos_token=\"<eos>\"\n)\n\nenglish = Field(\n    tokenize=tokenize_english,\n    lower=True,\n    init_token=\"<sos>\",\n    eos_token=\"<eos>\"\n)\n\nfields = [('trg', english), ('src', spanish)]","metadata":{"id":"usZnJlsdqHSO","execution":{"iopub.status.busy":"2022-03-28T18:53:20.213352Z","iopub.execute_input":"2022-03-28T18:53:20.213623Z","iopub.status.idle":"2022-03-28T18:53:20.219773Z","shell.execute_reply.started":"2022-03-28T18:53:20.213588Z","shell.execute_reply":"2022-03-28T18:53:20.219103Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"train_data, valid_data, test_data = TabularDataset.splits(\n    path='../input/mt-datasets',\n    train='train.csv',\n    validation='validation.csv',\n    test='test.csv',\n    format='csv',\n    fields=fields\n)","metadata":{"id":"S7bvV7xii85j","execution":{"iopub.status.busy":"2022-03-28T18:53:20.222162Z","iopub.execute_input":"2022-03-28T18:53:20.222579Z","iopub.status.idle":"2022-03-28T18:53:35.763289Z","shell.execute_reply.started":"2022-03-28T18:53:20.222541Z","shell.execute_reply":"2022-03-28T18:53:35.762522Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"print(f\"Registros datos de entrenamiento: {len(train_data)}\")\nprint(f\"Registros datos de validacion: {len(valid_data)}\")\nprint(f\"Registros datos de prueba: {len(test_data)}\")","metadata":{"id":"uWNfBDHboqij","outputId":"02214ddd-0844-4469-c039-ee0fbd6c6ea1","execution":{"iopub.status.busy":"2022-03-28T18:53:35.764575Z","iopub.execute_input":"2022-03-28T18:53:35.764816Z","iopub.status.idle":"2022-03-28T18:53:35.772651Z","shell.execute_reply.started":"2022-03-28T18:53:35.764783Z","shell.execute_reply":"2022-03-28T18:53:35.771812Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"for e in train_data[:4]:\n  print(e.src, '\\n', e.trg)\n  print()","metadata":{"id":"i6BEEurRdjOt","outputId":"ea6d62b1-9437-4774-dc97-d27830506800","execution":{"iopub.status.busy":"2022-03-28T18:53:35.774111Z","iopub.execute_input":"2022-03-28T18:53:35.774379Z","iopub.status.idle":"2022-03-28T18:53:35.783373Z","shell.execute_reply.started":"2022-03-28T18:53:35.774325Z","shell.execute_reply":"2022-03-28T18:53:35.782479Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"spanish.build_vocab(train_data, max_size=10000, min_freq=3)\nenglish.build_vocab(train_data, max_size=10000, min_freq=3)","metadata":{"id":"WOp1LuX2q0S0","execution":{"iopub.status.busy":"2022-03-28T18:53:35.784608Z","iopub.execute_input":"2022-03-28T18:53:35.785060Z","iopub.status.idle":"2022-03-28T18:53:36.674169Z","shell.execute_reply.started":"2022-03-28T18:53:35.785022Z","shell.execute_reply":"2022-03-28T18:53:36.673420Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"print(f\"Tokens únicos en el vocabulario del idioma alemán: {len(spanish.vocab):,}\")\nprint(f\"Tokens únicos en el vocabulario del idioma inglés: {len(english.vocab):,}\")","metadata":{"id":"xP82Fjt4rF6d","outputId":"46026aaa-5d6a-45f2-d805-41545cf15eab","execution":{"iopub.status.busy":"2022-03-28T18:53:36.675469Z","iopub.execute_input":"2022-03-28T18:53:36.675715Z","iopub.status.idle":"2022-03-28T18:53:36.683430Z","shell.execute_reply.started":"2022-03-28T18:53:36.675682Z","shell.execute_reply":"2022-03-28T18:53:36.680696Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"## Creación de iteradores","metadata":{"id":"ys3Qw2Vqo7-O"}},{"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Device: {device}\")\n\nBATCH_SIZE = 32\n\ntrain_iterator, valid_iterator, test_iterator = BucketIterator.splits((train_data, valid_data, test_data), \n                                                                      batch_size = BATCH_SIZE, \n                                                                      sort_within_batch=True,\n                                                                      sort_key=lambda x: len(x.src),\n                                                                      device = device)","metadata":{"id":"jzsPypdSo_xb","outputId":"1867e162-e5d7-469b-a83e-c2d7ac4ffa72","execution":{"iopub.status.busy":"2022-03-28T18:53:36.685169Z","iopub.execute_input":"2022-03-28T18:53:36.685432Z","iopub.status.idle":"2022-03-28T18:53:36.694203Z","shell.execute_reply.started":"2022-03-28T18:53:36.685396Z","shell.execute_reply":"2022-03-28T18:53:36.693486Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"print (f\"{len(train_iterator.dataset):,} muestras en el conjunto de entrenamiento\")\nprint (f\"{len(valid_iterator.dataset):,} muestras en el conjunto de validación\")\nprint (f\"{len(test_iterator.dataset):,} muestras en el conjunto de prueba\\n\")\n\nprint (f\"{len(train_iterator):,} batches de tamaño {BATCH_SIZE} en el conjunto de entrenamiento\")\nprint (f\"{len(valid_iterator):,} batches de tamaño {BATCH_SIZE} en el conjunto de validación\")\nprint (f\"{len(test_iterator):,} batches de tamaño {BATCH_SIZE} en el conjunto de prueba\")","metadata":{"id":"PfDOCTUgvRC8","outputId":"c00d8950-d2c9-40c6-8af4-f5c435d4f713","execution":{"iopub.status.busy":"2022-03-28T18:53:36.697627Z","iopub.execute_input":"2022-03-28T18:53:36.697827Z","iopub.status.idle":"2022-03-28T18:53:36.705046Z","shell.execute_reply.started":"2022-03-28T18:53:36.697801Z","shell.execute_reply":"2022-03-28T18:53:36.704241Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"sent_len_ger, sent_len_eng = [], []\nfor i, data in enumerate(train_data): \n    sent_len_ger.append(len(data.src))\n    sent_len_eng.append(len(data.trg))\n    if i < 10 :\n        print(f\"Español: {' '.join(data.src)} Length: {len(data.src)}\")\n        print(f\"Inglés: {' '.join(data.trg)} Length: {len(data.trg)}\\n\")\n\nprint(f\"Oración con longitud máxima y mínima en alemán:\\t{max(sent_len_ger)}\\t{min(sent_len_ger)}\")\nprint(f\"Oración con longitud máxima y mínima en inglés:\\t{max(sent_len_eng)}\\t{min(sent_len_eng)}\")","metadata":{"id":"LVlh7ZrnwCQt","outputId":"1b7ff7d1-c12b-4606-b54c-ae0a7d80d76c","execution":{"iopub.status.busy":"2022-03-28T18:53:36.706263Z","iopub.execute_input":"2022-03-28T18:53:36.707073Z","iopub.status.idle":"2022-03-28T18:53:36.834271Z","shell.execute_reply.started":"2022-03-28T18:53:36.707036Z","shell.execute_reply":"2022-03-28T18:53:36.833469Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":"## Definir el modelo Encoder-Decoder\nEn principio se define el encoder, que es quien recibe las entradas. En este caso las entradas corresponden al idioma Alemán.","metadata":{"id":"z2ySNE_xj5A6"}},{"cell_type":"markdown","source":"### Encoder","metadata":{"id":"X-SAcCnV2hjn"}},{"cell_type":"code","source":"# definir el encoder\nclass Encoder(nn.Module):\n    def __init__(self, input_size, embedding_size, hidden_size, num_layers, p):\n        super(Encoder, self).__init__()\n\n        # Size of the one hot vectors that will be the input to the encoder\n        self.input_size = input_size\n\n        # Output size of the word embedding NN\n        self.embedding_size = embedding_size\n\n        # Dimension of the NN's inside the lstm cell/ (hs,cs)'s dimension.\n        self.hidden_size = hidden_size\n\n        # Number of layers in the lstm\n        self.num_layers = num_layers\n\n        # Regularization parameter\n        self.dropout = nn.Dropout(p)\n        self.tag = True\n\n        # Shape --------------------> (5376, 300) [input size, embedding dims]\n        self.embedding = nn.Embedding(self.input_size, self.embedding_size)\n        \n        # Shape -----------> (300, 2, 1024) [embedding dims, hidden size, num layers]\n        self.LSTM = nn.LSTM(self.embedding_size, hidden_size, num_layers, dropout = p)\n\n    # Shape of x (26, 32) [Sequence_length, batch_size]\n    def forward(self, x):\n        # Shape -----------> (26, 32, 300) [Sequence_length , batch_size , embedding dims]\n        embedding = self.dropout(self.embedding(x))\n        \n        # Shape --> outputs (26, 32, 1024) [Sequence_length , batch_size , hidden_size]\n        # Shape --> (hs, cs) (2, 32, 1024) , (2, 32, 1024) [num_layers, batch_size size, hidden_size]\n        outputs, (hidden_state, cell_state) = self.LSTM(embedding)\n\n        return hidden_state, cell_state","metadata":{"id":"ra4OtYtjjzWq","execution":{"iopub.status.busy":"2022-03-28T18:53:36.835419Z","iopub.execute_input":"2022-03-28T18:53:36.835662Z","iopub.status.idle":"2022-03-28T18:53:36.843191Z","shell.execute_reply.started":"2022-03-28T18:53:36.835630Z","shell.execute_reply":"2022-03-28T18:53:36.842298Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"# instanciar el encoder\ninput_size_encoder = len(spanish.vocab)\nencoder_embedding_size = 300\nhidden_size = 1024\nnum_layers = 2\nencoder_dropout = float(0.5)\n\nencoder = Encoder(\n    input_size_encoder, \n    encoder_embedding_size,\n    hidden_size, \n    num_layers, \n    encoder_dropout\n).to(device)\n\nprint(encoder)","metadata":{"id":"QI-yi_yz3LTG","outputId":"4abbf4e2-e748-4270-c1e5-9a9226333732","execution":{"iopub.status.busy":"2022-03-28T18:53:36.844660Z","iopub.execute_input":"2022-03-28T18:53:36.844935Z","iopub.status.idle":"2022-03-28T18:53:45.383269Z","shell.execute_reply.started":"2022-03-28T18:53:36.844894Z","shell.execute_reply":"2022-03-28T18:53:45.382465Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":"### Decoder","metadata":{"id":"byJreosO2kDA"}},{"cell_type":"code","source":"class Decoder(nn.Module):\n    def __init__(self, input_size, embedding_size, hidden_size, num_layers, p, output_size):\n        super(Decoder, self).__init__()\n\n        # Size of the one hot vectors that will be the input to the encoder\n        self.input_size = input_size\n\n        # Output size of the word embedding NN\n        self.embedding_size = embedding_size\n\n        # Dimension of the NN's inside the lstm cell/ (hs,cs)'s dimension.\n        self.hidden_size = hidden_size\n\n        # Number of layers in the lstm\n        self.num_layers = num_layers\n\n        # Size of the one hot vectors that will be the output to the encoder (English Vocab Size)\n        self.output_size = output_size\n\n        # Regularization parameter\n        self.dropout = nn.Dropout(p)\n        self.tag = True\n\n        # Shape --------------------> (5376, 300) [input size, embedding dims]\n        self.embedding = nn.Embedding(self.input_size, self.embedding_size)\n\n        # Shape -----------> (300, 2, 1024) [embedding dims, hidden size, num layers]\n        self.LSTM = nn.LSTM(self.embedding_size, hidden_size, num_layers, dropout = p)\n\n        # Shape -----------> (1024, 4556) [embedding dims, hidden size, num layers]\n        self.fc = nn.Linear(self.hidden_size, self.output_size)\n\n    # Shape of x (32) [batch_size]\n    def forward(self, x, hidden_state, cell_state):\n\n        # Shape of x (1, 32) [1, batch_size]\n        x = x.unsqueeze(0)\n\n        # Shape -----------> (1, 32, 300) [1, batch_size, embedding dims]\n        embedding = self.dropout(self.embedding(x))\n\n        # Shape --> outputs (1, 32, 1024) [1, batch_size , hidden_size]\n        # Shape --> (hs, cs) (2, 32, 1024) , (2, 32, 1024) [num_layers, batch_size size, hidden_size] (passing encoder's hs, cs - context vectors)\n        outputs, (hidden_state, cell_state) = self.LSTM(embedding, (hidden_state, cell_state))\n\n        # Shape --> predictions (1, 32, 4556) [ 1, batch_size , output_size]\n        predictions = self.fc(outputs)\n\n        # Shape --> predictions (32, 4556) [batch_size , output_size]\n        predictions = predictions.squeeze(0)\n\n        return predictions, hidden_state, cell_state","metadata":{"id":"hiVOtttU2ljA","execution":{"iopub.status.busy":"2022-03-28T18:53:45.384703Z","iopub.execute_input":"2022-03-28T18:53:45.385248Z","iopub.status.idle":"2022-03-28T18:53:45.395090Z","shell.execute_reply.started":"2022-03-28T18:53:45.385210Z","shell.execute_reply":"2022-03-28T18:53:45.394388Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"# instanciar el decoder\ninput_size_decoder = len(english.vocab) \ndecoder_embedding_size = 300\nhidden_size = 1024\nnum_layers = 2\ndecoder_dropout = float(0.5)\noutput_size = len(english.vocab)\n\ndecoder = Decoder(\n    input_size_decoder, \n    decoder_embedding_size,\n    hidden_size, num_layers, \n    decoder_dropout, output_size\n).to(device)\n\nprint(decoder)","metadata":{"id":"P0XJQ5B32_Iq","outputId":"fc15c881-eb0e-4314-d437-bcca906dfaf1","execution":{"iopub.status.busy":"2022-03-28T18:53:45.396432Z","iopub.execute_input":"2022-03-28T18:53:45.396715Z","iopub.status.idle":"2022-03-28T18:53:45.622369Z","shell.execute_reply.started":"2022-03-28T18:53:45.396676Z","shell.execute_reply":"2022-03-28T18:53:45.621597Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"markdown","source":"## Definir el modelo Seq2Seq\n","metadata":{"id":"WGIJJlwd3z55"}},{"cell_type":"code","source":"# definir el modelo seq2seq que contiene al encoder y decoder\nclass Seq2Seq(nn.Module):\n    def __init__(self, Encoder, Decoder):\n        super(Seq2Seq, self).__init__()\n        self.Encoder = Encoder\n        self.Decoder = Decoder\n\n    def forward(self, source, target, tfr=0.5):\n        # Shape - Source : (10, 32) [(Sentence length German + some padding), Number of Sentences]\n        batch_size = source.shape[1]\n\n        # Shape - Source : (14, 32) [(Sentence length English + some padding), Number of Sentences]\n        target_len = target.shape[0]\n        target_vocab_size = len(english.vocab)\n        \n        # Shape --> outputs (14, 32, 5766) \n        outputs = torch.zeros(target_len, batch_size, target_vocab_size).to(device)\n\n        # Shape --> (hs, cs) (2, 32, 1024) ,(2, 32, 1024) [num_layers, batch_size size, hidden_size] (contains encoder's hs, cs - context vectors)\n        hidden_state_encoder, cell_state_encoder = self.Encoder(source)\n\n        # Shape of x (32 elements)\n        x = target[0] # Trigger token <SOS>\n\n        for i in range(1, target_len):\n            # Shape --> output (32, 5766) \n            output, hidden_state_decoder, cell_state_decoder = self.Decoder(x, hidden_state_encoder, cell_state_encoder)\n            outputs[i] = output\n            best_guess = output.argmax(1) # 0th dimension is batch size, 1st dimension is word embedding\n            x = target[i] if random.random() < tfr else best_guess # Either pass the next word correctly from the dataset or use the earlier predicted word\n\n        # Shape --> outputs (14, 32, 5766) \n        return outputs","metadata":{"id":"WTAmAguC35_p","execution":{"iopub.status.busy":"2022-03-28T18:53:45.623737Z","iopub.execute_input":"2022-03-28T18:53:45.624258Z","iopub.status.idle":"2022-03-28T18:53:45.633348Z","shell.execute_reply.started":"2022-03-28T18:53:45.624122Z","shell.execute_reply":"2022-03-28T18:53:45.632514Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"# instanciar el modelo principal (seq2seq)\nmodel = Seq2Seq(encoder, decoder)\nprint(model)","metadata":{"id":"luF6fSG64PNg","outputId":"6e562985-79ba-429e-8f77-dbfe603656e8","execution":{"iopub.status.busy":"2022-03-28T18:53:45.634523Z","iopub.execute_input":"2022-03-28T18:53:45.634895Z","iopub.status.idle":"2022-03-28T18:53:45.647785Z","shell.execute_reply.started":"2022-03-28T18:53:45.634859Z","shell.execute_reply":"2022-03-28T18:53:45.647104Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"markdown","source":"## Entrenamiento","metadata":{"id":"YukK1vv846Kx"}},{"cell_type":"markdown","source":"### Hiperparámetros","metadata":{"id":"Ku6rk_hG6kpx"}},{"cell_type":"code","source":"# Hyperparámetros\nlearning_rate = 0.001\nstep = 0\noptimizer = optim.Adam(model.parameters(), lr=learning_rate)\npad_idx = english.vocab.stoi[\"<pad>\"] # obtener id del relleno\ncriterion = nn.CrossEntropyLoss(ignore_index=pad_idx) # omitir calcular elementos con relleno en el backpropagation","metadata":{"id":"41IiVXYD6we4","execution":{"iopub.status.busy":"2022-03-28T18:53:45.649165Z","iopub.execute_input":"2022-03-28T18:53:45.649597Z","iopub.status.idle":"2022-03-28T18:53:45.655818Z","shell.execute_reply.started":"2022-03-28T18:53:45.649563Z","shell.execute_reply":"2022-03-28T18:53:45.654900Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"def translate_sentence(model, sentence, spanish, english, device, max_length=50):\n    spacy_ger = spacy.load(\"es_core_news_sm\")\n\n    if type(sentence) == str:\n        tokens = [token.text.lower() for token in spacy_ger(sentence)]\n    else:\n        tokens = [token.lower() for token in sentence]\n    tokens.insert(0, spanish.init_token)\n    tokens.append(spanish.eos_token)\n    text_to_indices = [spanish.vocab.stoi[token] for token in tokens]\n    sentence_tensor = torch.LongTensor(text_to_indices).unsqueeze(1).to(device)\n\n    # Build encoder hidden, cell state\n    with torch.no_grad():\n        hidden, cell = model.Encoder(sentence_tensor)\n\n    outputs = [english.vocab.stoi[\"<sos>\"]]\n\n    for _ in range(max_length):\n        previous_word = torch.LongTensor([outputs[-1]]).to(device)\n\n        with torch.no_grad():\n            output, hidden, cell = model.Decoder(previous_word, hidden, cell)\n            best_guess = output.argmax(1).item()\n\n        outputs.append(best_guess)\n\n        # Model predicts it's the end of the sentence\n        if output.argmax(1).item() == english.vocab.stoi[\"<eos>\"]:\n            break\n\n    translated_sentence = [english.vocab.itos[idx] for idx in outputs]\n    return translated_sentence[1:]","metadata":{"id":"25Ett4aF76MK","execution":{"iopub.status.busy":"2022-03-28T19:11:32.779219Z","iopub.execute_input":"2022-03-28T19:11:32.779492Z","iopub.status.idle":"2022-03-28T19:11:32.789209Z","shell.execute_reply.started":"2022-03-28T19:11:32.779463Z","shell.execute_reply":"2022-03-28T19:11:32.788495Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"def bleu(data, model, spanish, english, device):\n    \"\"\"Método para medir el rendimiento del modelo\"\"\"\n    targets = []\n    outputs = []\n\n    for example in data:\n        src = vars(example)[\"src\"]\n        trg = vars(example)[\"trg\"]\n\n        prediction = translate_sentence(model, src, spanish, english, device)\n        prediction = prediction[:-1]  # remove <eos> token\n\n        targets.append([trg])\n        outputs.append(prediction)\n\n    return bleu_score(outputs, targets)","metadata":{"id":"WYhNHMum7UbB","execution":{"iopub.status.busy":"2022-03-28T18:53:45.669589Z","iopub.execute_input":"2022-03-28T18:53:45.670070Z","iopub.status.idle":"2022-03-28T18:53:45.680345Z","shell.execute_reply.started":"2022-03-28T18:53:45.670033Z","shell.execute_reply":"2022-03-28T18:53:45.679619Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"def checkpoint_and_save(model, best_loss, epoch, optimizer, epoch_loss):\n    print('saving')\n    print()\n    state = {'model': model,'best_loss': best_loss,'epoch': epoch,'rng_state': torch.get_rng_state(), 'optimizer': optimizer.state_dict(),}\n    torch.save(state, 'checkpoint-NMT.ckpt')\n    torch.save(model.state_dict(),'checkpoint-NMT-SD.ckpt')","metadata":{"id":"y8ExkUuK8tly","execution":{"iopub.status.busy":"2022-03-28T19:03:26.804606Z","iopub.execute_input":"2022-03-28T19:03:26.805159Z","iopub.status.idle":"2022-03-28T19:03:26.810167Z","shell.execute_reply.started":"2022-03-28T19:03:26.805113Z","shell.execute_reply":"2022-03-28T19:03:26.809391Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"def epoch_time(start_time, end_time):\n    \"\"\"Obtener el tiempo en minutos y segundos\"\"\"\n    elapsed_time = end_time - start_time\n    elapsed_mins = int(elapsed_time / 60)\n    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n    return elapsed_mins, elapsed_secs","metadata":{"id":"_Dn3wtxKCohT","execution":{"iopub.status.busy":"2022-03-28T18:53:45.691039Z","iopub.execute_input":"2022-03-28T18:53:45.691372Z","iopub.status.idle":"2022-03-28T18:53:45.701478Z","shell.execute_reply.started":"2022-03-28T18:53:45.691336Z","shell.execute_reply":"2022-03-28T18:53:45.700686Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"import time\n\nbest_valid_loss = float('inf')\nnum_epochs = 50\n# best_loss = 999999\n# best_epoch = -1\nsentence1 = \"esta es mi primera oracion en español\"\nepoch_loss_list = []\nts1 = []\n\nstart_total_time = time.time() # inicializar \nfor epoch in range(num_epochs):\n    epoch_loss = 0.0\n    # calcular el tiempo que tarda cada epoca\n    start_time = time.time() # inicializar\n\n    model.train(True)\n    for batch_idx, batch in enumerate(train_iterator):\n        input = batch.src.to(device)\n        target = batch.trg.to(device)\n\n        # Pass the input and target for model's forward method\n        output = model(input, target)\n        output = output[1:].reshape(-1, output.shape[2])\n        target = target[1:].reshape(-1)\n\n        # Clear the accumulating gradients\n        optimizer.zero_grad()\n\n        # Calculate the loss value for every epoch\n        loss = criterion(output, target)\n\n        # Calculate the gradients for weights & biases using back-propagation\n        loss.backward()\n\n        # Clip the gradient value is it exceeds > 1\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n\n        # Update the weights values using the gradients we calculated using bp \n        optimizer.step()\n        step += 1\n        epoch_loss += loss.item()\n        # writer.add_scalar(\"Training loss\", loss, global_step=step)\n\n    # if epoch_loss < best_loss:\n    #   best_loss = epoch_loss\n    #   best_epoch = epoch\n    #   checkpoint_and_save(model, best_loss, epoch, optimizer, epoch_loss) \n    #   if ((epoch - best_epoch) >= 10):\n    #     print(\"no improvement in 10 epochs, break\")\n    #     break\n\n    if epoch_loss < best_valid_loss: # guardar el mejor modelo\n        best_valid_loss = epoch_loss\n        checkpoint_and_save(model, best_valid_loss, epoch, optimizer, epoch_loss) \n      \n    tmp_loss = epoch_loss / len(train_iterator)\n    epoch_loss_list.append(tmp_loss)\n    \n    end_time = time.time() # obtener el tiempo final\n    # obtener los minutos y segundos del tiempo de ejecución\n    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n\n    # ----------------------------------------------------------------------------------\n    # evaluación\n    model.eval()\n    with torch.no_grad():\n        translated_sentence1 = translate_sentence(model, sentence1, spanish, english, device, max_length=50)\n        print(f\"Translated example sentence 1:\\n{' '.join(translated_sentence1)}\")\n        ts1.append(translated_sentence1)\n    # ----------------------------------------------------------------------------------\n\n    print(f\"Epoch - {epoch+1} / {num_epochs} | Epoch Time:\\t{epoch_mins}m {epoch_secs}s | Epoch_Loss:\\t{tmp_loss:.3f}\")\n    print()\n  \nprint(epoch_loss / len(train_iterator))\n\nscore = bleu(test_data[1:100], model, spanish, english, device)\nprint(f\"Bleu score {score*100:.2f}\")\n\nend_total_time = time.time() # obtener el tiempo final\nepoch_mins, epoch_secs = epoch_time(start_total_time, end_total_time)\nprint (f\"\\nTotal time for training model:\\t{epoch_mins}m {epoch_secs}s\")","metadata":{"id":"ZyTd4Y6u476I","outputId":"1844227e-692e-4b42-f712-6944af13dbb9","execution":{"iopub.status.busy":"2022-03-28T19:11:36.653270Z","iopub.execute_input":"2022-03-28T19:11:36.655020Z","iopub.status.idle":"2022-03-28T21:53:42.005447Z","shell.execute_reply.started":"2022-03-28T19:11:36.654970Z","shell.execute_reply":"2022-03-28T21:53:42.004455Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"markdown","source":"<pre>\nsaving\n\nTranslated example sentence 1:\na blue a a a a game of a group of a swing thin school gyro touch heels short awaits demonstrates wheelchairs wires expanse oxford loading oxford removing tethered vegetation sunrise multicolor cardigan janitor tomato cubicle projected secured snap surgery amid fancy bites apart attentively down challenge religious things crate\nEpoch - 1 / 30 | Epoch Time:\t3m 29s | Epoch_Loss:\t1.926\n\nsaving\n\nTranslated example sentence 1:\na blue in a a a game of red striped hardwood discuss yelling tulips pause waited ally happening slam slide passersby sweatshirt violinist pack crotch rack backhoe propane attaches asking texas dunk turbans aqua tye tye ripped laundromat law law bunny landed procession sheepdog solitary 30 bagpipe law law straining\nEpoch - 2 / 30 | Epoch Time:\t3m 29s | Epoch_Loss:\t1.891\n\nsaving\n\nTranslated example sentence 1:\na blue is on a a down some a baseball white five used used zone purse twirl mustachioed she can contraption mad mad weathered excitement heavyset gyro bookstore janitor law classic law scissors bookstore cars wakeboard janitor law law enforcement law law pajamas miami junk metallic reach onlooker crab three\nEpoch - 3 / 30 | Epoch Time:\t3m 30s | Epoch_Loss:\t1.859\n\nsaving\n\nTranslated example sentence 1:\na window a a a small a small a small a girls smiles force french constructed honor related participants aerial participants california stay healthcare fog fog fog graying judges major streams how diverse box shop trash rollerblader rollerblader elbow dozen twirls rollerblader solitary wicker forested shelter stall mountaintop scenic act\nEpoch - 4 / 30 | Epoch Time:\t3m 29s | Epoch_Loss:\t1.829\n\nsaving\n\nTranslated example sentence 1:\na blue on a a a small <unk> a laugh group of a good length climb m rod dragon parts mustard return shaded handbag electrician cycles briefcase topped damaged rainstorm solitary claus beige hills preparation teal bearer mustard years shaded outfits spectacular shaded projected pitched metropolitan shaded create fills pokes\nEpoch - 5 / 30 | Epoch Time:\t3m 29s | Epoch_Loss:\t1.797\n\nsaving\n\nTranslated example sentence 1:\na blue a a group of worker sitting road abandoned moving vietnamese personnel breaking congregating dumpster college pub seemingly paneled monkey honor gyro features studies ohio strikes equipped butchering competes on a a good picture of straw stone figure points dough with resting himself with tasty bones features sprinkled excitedly\nEpoch - 6 / 30 | Epoch Time:\t3m 28s | Epoch_Loss:\t1.772\n\nsaving\n\nTranslated example sentence 1:\na blue a a a football of young men carve cord technique memorial bears squat features bears lounge flames waring bears paneled doo motorboat tye sprays air track . <eos>\nEpoch - 7 / 30 | Epoch Time:\t3m 29s | Epoch_Loss:\t1.750\n\nsaving\n\nTranslated example sentence 1:\na blue . <eos>\nEpoch - 8 / 30 | Epoch Time:\t3m 29s | Epoch_Loss:\t1.732\n\nsaving\n\nTranslated example sentence 1:\na blue in a a group of smoking a red boat <eos>\nEpoch - 9 / 30 | Epoch Time:\t3m 29s | Epoch_Loss:\t1.705\n\nsaving\n\nTranslated example sentence 1:\na blue in the <eos>\nEpoch - 10 / 30 | Epoch Time:\t3m 29s | Epoch_Loss:\t1.694\n\nsaving\n\nTranslated example sentence 1:\na blue in a a large crowd this ready picture picture of a cement baggage road children pain fists pain veteran pain pain veteran unique toe drainage used cry chart where surfers elevator elevator primitive primitive honor toss features unfinished cabin socks opens salad walker stride mark butt crying barber\nEpoch - 11 / 30 | Epoch Time:\t3m 29s | Epoch_Loss:\t1.656\n\nsaving\n\nTranslated example sentence 1:\na blue on the inside building . <eos>\nEpoch - 12 / 30 | Epoch Time:\t3m 29s | Epoch_Loss:\t1.639\n\nsaving\n\nTranslated example sentence 1:\na blue in a group of enjoys dirty displays design tool tool . <eos>\nEpoch - 13 / 30 | Epoch Time:\t3m 28s | Epoch_Loss:\t1.629\n\nsaving\n\nTranslated example sentence 1:\na blue standing on on a the frisbee . <eos>\nEpoch - 14 / 30 | Epoch Time:\t3m 29s | Epoch_Loss:\t1.585\n\nsaving\n\nTranslated example sentence 1:\na blue on the counter belts portable ticket jumpsuits sprinkled jogger beagle toyota karaoke bowler metropolitan corral rollerblader rollerblader dozens sprinkles forrest aerial left . <eos>\nEpoch - 15 / 30 | Epoch Time:\t3m 28s | Epoch_Loss:\t1.575\n\nsaving\n\nTranslated example sentence 1:\na blue standing on on a a a small crowd <eos>\nEpoch - 16 / 30 | Epoch Time:\t3m 28s | Epoch_Loss:\t1.560\n\nsaving\n\nTranslated example sentence 1:\na blue blue standing in on horses three animal camouflage horns corral scantily heavyset frozen defends jack find bulls angry fit short business business wear decorations laundromat safe suspended up belts obama tasty flowery hikes pony pony latino jet schoolgirl law operated ways stacking views an starring fork instructor rainstorm\nEpoch - 17 / 30 | Epoch Time:\t3m 28s | Epoch_Loss:\t1.539\n\nsaving\n\nTranslated example sentence 1:\na blue in the water with aprons panda observing microscopes shooting a lap shot of gets bread help cup pot board . <eos>\nEpoch - 18 / 30 | Epoch Time:\t3m 28s | Epoch_Loss:\t1.535\n\nsaving\n\nTranslated example sentence 1:\na window . <eos>\nEpoch - 19 / 30 | Epoch Time:\t3m 28s | Epoch_Loss:\t1.522\n\nsaving\n\nTranslated example sentence 1:\na blue standing on a an three leg picture ready into panda depicting pigtails under clutching carrying still food a girl who yellow snaps mcdonalds waterskier law law waterskier waterskier law law warehouse maneuver firetrucks painters painters texas healthcare better better dozen crouches priest lets sooners personnel chart begging paying\nEpoch - 20 / 30 | Epoch Time:\t3m 28s | Epoch_Loss:\t1.490\n\nTranslated example sentence 1:\na blue while one congregate gliding signing reflective vests butchering names names tutu barriers mustard visible follow grabbing yelling island zebra tattered attaches bananas field . tan range art sits karate pose pose cross painter painter dugout law braces toss ragged ref dozens texts tiles elevator toss troops crayon jumpsuits\nEpoch - 21 / 30 | Epoch Time:\t3m 26s | Epoch_Loss:\t1.495\n\nsaving\n\nTranslated example sentence 1:\na blue standing on <eos>\nEpoch - 22 / 30 | Epoch Time:\t3m 28s | Epoch_Loss:\t1.479\n\nsaving\n\nTranslated example sentence 1:\na blue while on displays some blown draped place in some long feet foreign studio lines leaves . <eos>\nEpoch - 23 / 30 | Epoch Time:\t3m 29s | Epoch_Loss:\t1.448\n\nTranslated example sentence 1:\na blue standing on <eos>\nEpoch - 24 / 30 | Epoch Time:\t3m 26s | Epoch_Loss:\t1.448\n\nsaving\n\nTranslated example sentence 1:\na blue blue on the grass pool <eos>\nEpoch - 25 / 30 | Epoch Time:\t3m 29s | Epoch_Loss:\t1.437\n\nsaving\n\nTranslated example sentence 1:\na blue standing on the an three caught japanese suspended 6 descent lets using laptops . <eos>\nEpoch - 26 / 30 | Epoch Time:\t3m 28s | Epoch_Loss:\t1.413\n\nsaving\n\nTranslated example sentence 1:\na blue while on a mountain glass room bull outdoors . <eos>\nEpoch - 27 / 30 | Epoch Time:\t3m 29s | Epoch_Loss:\t1.398\n\nTranslated example sentence 1:\na blue is standing on <eos>\nEpoch - 28 / 30 | Epoch Time:\t3m 27s | Epoch_Loss:\t1.401\n\nsaving\n\nTranslated example sentence 1:\na blue on the bar military lights cigar leaving unique bamboo <unk> fountains garden . <eos>\nEpoch - 29 / 30 | Epoch Time:\t3m 29s | Epoch_Loss:\t1.393\n\nsaving\n\nTranslated example sentence 1:\na blue standing on <eos>\nEpoch - 30 / 30 | Epoch Time:\t3m 29s | Epoch_Loss:\t1.371\n\n1.3713945452834437\nBleu score 1.87\n\nTotal time for training model:\t107m 43s\n</pre>","metadata":{"id":"1988z5HbZG5b"}},{"cell_type":"markdown","source":"<pre>\nsaving\n\nTranslated example sentence 1:\na a a man . <eos>\nEpoch - 1 / 20 | Epoch Time:\t3m 29s | Epoch_Loss:\t4.719\n\nsaving\n\nTranslated example sentence 1:\na man in a a a a . <eos>\nEpoch - 2 / 20 | Epoch Time:\t3m 29s | Epoch_Loss:\t3.985\n\nsaving\n\nTranslated example sentence 1:\na man . <eos>\nEpoch - 3 / 20 | Epoch Time:\t3m 29s | Epoch_Loss:\t3.576\n\nsaving\n\nTranslated example sentence 1:\na man in a a . <eos>\nEpoch - 4 / 20 | Epoch Time:\t3m 29s | Epoch_Loss:\t3.333\n\nsaving\n\nTranslated example sentence 1:\na man in a a a a a a a a a a . <eos>\nEpoch - 5 / 20 | Epoch Time:\t3m 30s | Epoch_Loss:\t3.132\n\nsaving\n\nTranslated example sentence 1:\na man in a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a\nEpoch - 6 / 20 | Epoch Time:\t3m 31s | Epoch_Loss:\t2.965\n\nsaving\n\nTranslated example sentence 1:\na blue man . <eos>\nEpoch - 7 / 20 | Epoch Time:\t3m 30s | Epoch_Loss:\t2.848\n\nsaving\n\nTranslated example sentence 1:\na man in a a a a a a a a a a a young a small . <eos>\nEpoch - 8 / 20 | Epoch Time:\t3m 31s | Epoch_Loss:\t2.721\n\nsaving\n\nTranslated example sentence 1:\na man in a a a a a a a a a a a a a a a a a a a a a a a a a a a a a child <eos>\nEpoch - 9 / 20 | Epoch Time:\t3m 29s | Epoch_Loss:\t2.605\n\nsaving\n\nTranslated example sentence 1:\na man in a a a a . <eos>\nEpoch - 10 / 20 | Epoch Time:\t3m 29s | Epoch_Loss:\t2.525\n\nsaving\n\nTranslated example sentence 1:\na blue man <eos>\nEpoch - 11 / 20 | Epoch Time:\t3m 29s | Epoch_Loss:\t2.432\n\nsaving\n\nTranslated example sentence 1:\na man in a a a <unk> a <unk> a <unk> a . <eos>\nEpoch - 12 / 20 | Epoch Time:\t3m 28s | Epoch_Loss:\t2.354\n\nsaving\n\nTranslated example sentence 1:\na blue a a a blue <eos>\nEpoch - 13 / 20 | Epoch Time:\t3m 29s | Epoch_Loss:\t2.286\n\nsaving\n\nTranslated example sentence 1:\na blue in a a a a a a a a a a a a a a large crowd of a a a red <unk> is hanging in the side the side of a a large group of a red of water <eos>\nEpoch - 14 / 20 | Epoch Time:\t3m 29s | Epoch_Loss:\t2.230\n\nsaving\n\nTranslated example sentence 1:\na blue in a a a a a a a a a a soccer of a a <unk> a a a <unk> white race is taking a a a a race a <unk> a <unk> piece of a jumper burger sitting in the real troops real reclines sits in side\nEpoch - 15 / 20 | Epoch Time:\t3m 28s | Epoch_Loss:\t2.188\n\nsaving\n\nTranslated example sentence 1:\na blue a man is <eos>\nEpoch - 16 / 20 | Epoch Time:\t3m 28s | Epoch_Loss:\t2.139\n\nsaving\n\nTranslated example sentence 1:\na blue in a a a small <unk> a <unk> a giant crowd of bright white aim handicapped short stripes m softball pose . <eos>\nEpoch - 17 / 20 | Epoch Time:\t3m 29s | Epoch_Loss:\t2.075\n\nsaving\n\nTranslated example sentence 1:\na blue blue a a <unk> <eos>\nEpoch - 18 / 20 | Epoch Time:\t3m 28s | Epoch_Loss:\t2.033\n\nsaving\n\nTranslated example sentence 1:\na blue a blue a race of a football race of a football . <eos>\nEpoch - 19 / 20 | Epoch Time:\t3m 28s | Epoch_Loss:\t1.991\n\nsaving\n\nTranslated example sentence 1:\na blue a blue <unk> looking like a a small <unk> stand in the <unk> striped top bathing vibrant soda pillar boys painters navigating rack last moose grasping lively save shadowed pro bush shaded winter skirt wicker slightly muscular statute special aim posters thumb begins chubby crystal plane dunes .\nEpoch - 20 / 20 | Epoch Time:\t3m 28s | Epoch_Loss:\t1.962\n\n1.961763057056835\nBleu score 2.50\n\nTotal time for training model:\t72m 28s\n</pre>","metadata":{"id":"cMeXVn-fo7ym"}},{"cell_type":"code","source":"# graficar las perdidas\nimport matplotlib.pyplot as plt\n\nprint (f\"Epochs: {len(epoch_loss_list)}\")\n\n# Graficar accuracy y loss\nfig, ax = plt.subplots(figsize=(15,10))\nplt.plot(epoch_loss_list, label='Training loss', marker='o', color='orange')\nplt.title('Losses', fontsize=15)\nplt.xlabel('Epoch', fontsize=14)\nplt.ylabel('Loss', fontsize=14)\nplt.xticks(fontsize=12)\nplt.yticks(fontsize=12)\nplt.grid()\nplt.legend()\nplt.savefig('loss.png')\nplt.show()","metadata":{"id":"Drl-0QIhGtPt","execution":{"iopub.status.busy":"2022-03-28T22:06:00.745465Z","iopub.execute_input":"2022-03-28T22:06:00.745726Z","iopub.status.idle":"2022-03-28T22:06:01.074754Z","shell.execute_reply.started":"2022-03-28T22:06:00.745697Z","shell.execute_reply":"2022-03-28T22:06:01.074010Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"markdown","source":"### Descargar el mejor modelo entrenado","metadata":{"id":"TJ9Bm2eNq95_"}},{"cell_type":"code","source":"checkpoint = 'checkpoint-NMT-SD.ckpt'\n\n# descargar el mejor modelo\nfiles.download(checkpoint)\n\n# descargar gráfica de perdidas\nfiles.download('loss.png')","metadata":{"id":"u78wkSGbrENX","execution":{"iopub.status.busy":"2022-03-28T22:06:25.497049Z","iopub.execute_input":"2022-03-28T22:06:25.497302Z","iopub.status.idle":"2022-03-28T22:06:25.521402Z","shell.execute_reply.started":"2022-03-28T22:06:25.497274Z","shell.execute_reply":"2022-03-28T22:06:25.519896Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":" ","metadata":{"id":"1lwi08F9v6UR"},"execution_count":null,"outputs":[]}]}