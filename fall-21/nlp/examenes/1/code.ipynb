{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examen 1: NLP\n",
    "\n",
    "- Néstor I. Martínez Ostoa\n",
    "- Procesamiento de Lenguaje Natural\n",
    "- LCD, IIMAS, UNAM\n",
    "- Octubre 2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5572, 2)\n",
      "['ham' 'spam']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text target\n",
       "0  Go until jurong point, crazy.. Available only ...    ham\n",
       "1                      Ok lar... Joking wif u oni...    ham\n",
       "2  Free entry in 2 a wkly comp to win FA Cup fina...   spam\n",
       "3  U dun say so early hor... U c already then say...    ham\n",
       "4  Nah I don't think he goes to usf, he lives aro...    ham"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"spam.csv\")\n",
    "print(df.shape)\n",
    "print(df[\"target\"].unique())\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preguntas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. ¿Qué porcentaje de los documentos en spam.csv son spam?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.406317300789663\n"
     ]
    }
   ],
   "source": [
    "num_of_docs = df.shape[0]\n",
    "num_of_spam_docs = df[df[\"target\"] == \"spam\"].shape[0]\n",
    "num_of_ham_docs = df[df[\"target\"] == \"ham\"].shape[0]\n",
    "\n",
    "#print(num_of_docs, num_of_spam_docs, num_of_ham_docs)\n",
    "\n",
    "print((num_of_spam_docs / num_of_docs)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Dados los siguientes vectores, calcule la similitud coseno entre ellos:\n",
    "\n",
    "- $V_1=[1,0,-1,6,8]$\n",
    "- $V_2=[0,11,4,7,6]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(v1, v2):\n",
    "    v1 = np.array(v1)\n",
    "    v2 = np.array(v2)\n",
    "\n",
    "    dot_prod_of_vectors = np.dot(v1, v2)\n",
    "    norm_of_vectors = np.linalg.norm(v1) * np.linalg.norm(v2)\n",
    "\n",
    "    sim = round(dot_prod_of_vectors / norm_of_vectors, 4)\n",
    "\n",
    "    return sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5715\n"
     ]
    }
   ],
   "source": [
    "v1 = [1,0,-1,6,8]\n",
    "v2 = [0,11,4,7,6]\n",
    "print(cosine_similarity(v1, v2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. ¿Cuál es el número promedio de caracteres que no son palabras (cualquier cosa que no sea una letra, un dígito o un guión bajo) por documento para los documentos que no son spam y spam?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def non_word_char_count(doc):\n",
    "    count = 0\n",
    "    for c in doc:\n",
    "        if c.isalpha() or c.isdigit() or c == \"_\": \n",
    "            continue\n",
    "        count += 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ham: 17.29181347150259\n",
      "Spam: 29.042838018741634\n"
     ]
    }
   ],
   "source": [
    "non_word_ham = []\n",
    "non_word_spam = []\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    doc = row[\"text\"]\n",
    "    target = row[\"target\"]\n",
    "\n",
    "    non_words = non_word_char_count(doc)\n",
    "\n",
    "    if target == \"ham\":\n",
    "        non_word_ham.append(non_words)\n",
    "    else:\n",
    "        non_word_spam.append(non_words)\n",
    "\n",
    "non_word_ham = np.array(non_word_ham)\n",
    "non_word_spam = np.array(non_word_spam)\n",
    "\n",
    "print(f\"Ham: {np.mean(non_word_ham)}\")\n",
    "print(f\"Spam: {np.mean(non_word_spam)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. ¿Cuál es la longitud promedio de los documentos (número de caracteres) para los documentos spam y no spam?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spam: 138.8661311914324\n",
      "Ham: 71.02362694300518\n"
     ]
    }
   ],
   "source": [
    "def get_avg_len(target):\n",
    "    docs = df[df[\"target\"]==target][\"text\"].values\n",
    "    return np.mean([len(d) for d in docs])\n",
    "\n",
    "sl = get_avg_len(\"spam\")\n",
    "hl = get_avg_len(\"ham\")\n",
    "\n",
    "print(f\"Spam: {sl}\")\n",
    "print(f\"Ham: {hl}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Observe la siguiente imagen y utilice los vectores de palabras para predecir a cuál país pertenece el río Volga y cuál es la distancia euclidiana 'd' entre su vector de predicción y el vector correcto. Recuerde que V1-R1+R2=P2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean(A, B):\n",
    "    diff = A-B\n",
    "    d = np.sqrt(np.sum(diff*diff))\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4142135623730951\n"
     ]
    }
   ],
   "source": [
    "r1 = np.array([9,8])\n",
    "v1 = np.array([4,9])\n",
    "r2 = np.array([8,1])\n",
    "p2 = v1-r1+r2\n",
    "\n",
    "v2 = np.array([2,1])\n",
    "print(euclidean(v2, p2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. ¿Cuál es el número promedio de dígitos por documento para los documentos no spam y spam?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_avg_digits(docs):\n",
    "    digits = []\n",
    "    for doc in docs:\n",
    "        c = 0\n",
    "        for d in doc:\n",
    "            if d.isdigit(): c += 1\n",
    "        digits.append(c)\n",
    "    return np.mean(np.array(digits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spam: 15.759036144578314\n",
      "Ham: 0.2992746113989637\n"
     ]
    }
   ],
   "source": [
    "spam_docs = df[df[\"target\"]==\"spam\"][\"text\"].values\n",
    "ham_docs = df[df[\"target\"]==\"ham\"][\"text\"].values\n",
    "\n",
    "print(f\"Spam: {get_avg_digits(spam_docs)}\")\n",
    "print(f\"Ham: {get_avg_digits(ham_docs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Suponga que en el dataset de spams el 25%  de los email que son spam contiene la palabra “gane”. También se sabe que un total de 13% de emails en el dataset contienen “gane”, y que el 40% del número total de correos son spam. ¿Cuál es la probabilidad de que el siguiente correo sea spam?: “Gane 1,000 pesos enviando la palabra LOTERIA al 3344”\n",
    "\n",
    "---\n",
    "Definimos las siguientes variables aleatorias:\n",
    "\n",
    "- $X$: SPAM\n",
    "- $Y$: \"Gane\"\n",
    "\n",
    "Utilizando el teorema de Bayes: \n",
    "$$P(X|Y) = \\frac{P(Y|X)*P(X)}{P(Y)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7692307692307693\n"
     ]
    }
   ],
   "source": [
    "P_YX = 0.25\n",
    "P_X = 0.4\n",
    "P_Y = 0.13\n",
    "\n",
    "P_XY = (P_YX*P_X)/P_Y\n",
    "\n",
    "print(P_XY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Dado el siguiente documento, encontrar el modelo de lenguaje n-grama para n = 3 y encontrar ¿Cuántos 3-gramas de palabras existen en el documento?\n",
    "\n",
    "\n",
    "Abstract Language users never choose words randomly, and language is essentially non-random. Statistical hypothesis testing uses a null hypothesis, which posits randomness. Hence, when we look at linguistic phenomena in corpora, the null hypothesis will never be true. Moreover, where there is enough data, we shall (almost) always be able to establish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85\n"
     ]
    }
   ],
   "source": [
    "str_ = \"Abstract Language users never choose words randomly, and language is essentially non-random. Statistical hypothesis testing uses a null hypothesis, which posits randomness. Hence, when we look at linguistic phenomena in corpora, the null hypothesis will never be true. Moreover, where there is enough data, we shall (almost) always be able to establish.\"\n",
    "str_ = str_.replace(\",\", \"\")\n",
    "str_ = str_.replace(\"-\", \"\")\n",
    "str_ = str_.replace(\".\", \"\")\n",
    "str_ = str_.replace(\"(\", \"\")\n",
    "str_ = str_.replace(\")\", \"\")\n",
    "\n",
    "doc = str_.split(\" \")\n",
    "\n",
    "trigrams = []\n",
    "\n",
    "wc = 0\n",
    "trigram = \"\"\n",
    "for word in str_:\n",
    "    if wc < 3: \n",
    "        trigram += word + \" \"\n",
    "        wc += 1\n",
    "    else:\n",
    "        trigrams.append(trigram)\n",
    "        trigram = \"\"\n",
    "        wc = 0\n",
    "\n",
    "\n",
    "print(len(trigrams))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
